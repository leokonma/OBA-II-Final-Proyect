p.adjust.method = "bonferroni")
print(t)
# =========================================================
# 5) AUTOMATIC INTERPRETATION
# =========================================================
cat("\n===== INTERPRETATION (AUTO-GENERATED) =====\n")
interpret <- function(anova_res, kw_res, t_res, wilcox_res) {
# Detect significance
sig_anova  <- summary(anova_res)[[1]][["Pr(>F)"]][1] < 0.05
sig_kw     <- kw_res$p.value < 0.05
cat("\n--- Global Tests ---\n")
if (sig_anova) {
cat("ANOVA: Significant differences detected between GA, DE, PSO (p < 0.05).\n")
} else {
cat("ANOVA: No significant differences between GA, DE, PSO.\n")
}
if (sig_kw) {
cat("Kruskal–Wallis: Significant differences detected (p < 0.05).\n")
} else {
cat("Kruskal–Wallis: No significant differences found.\n")
}
cat("\n--- Pairwise Comparisons ---\n")
# Pairwise t-tests
cat("\nPairwise t-tests (Bonferroni-adjusted):\n")
print(t_res$p.value)
# Pairwise Wilcoxon tests
cat("\nPairwise Wilcoxon tests (Bonferroni-adjusted):\n")
print(wilcox_res$p.value)
cat("\n--- Summary Interpretation ---\n")
# Determine best method
best_means <- tapply(df$value, df$method, mean)
best_method <- names(which.min(best_means))
cat(sprintf("Mean performance (lower = better): GA = %.2f, DE = %.2f, PSO = %.2f\n",
best_means["GA"], best_means["DE"], best_means["PSO"]))
cat(sprintf("Best overall method: %s\n", best_method))
cat("\nInterpretation:\n")
if (sig_kw | sig_anova) {
cat(sprintf("There are statistically significant differences between at least two algorithms.\n%s appears to outperform the others on average.\n",
best_method))
} else {
cat("There are NO statistically significant differences between the algorithms.\nThey perform similarly.\n")
}
}
interpret(anova_res, kw, t, wilc)
plot(avg_GA, type="l", lwd=3, col="blue",
ylim = range(c(avg_GA, avg_DE, avg_PSO)),
xlab="Iteration", ylab="Average best",
main="Average Convergence (GA, DE, PSO)")
lines(avg_DE, lwd=3, col="darkgreen")
lines(avg_PSO, lwd=3, col="red")
legend("topright", legend=c("GA", "DE", "PSO"),
col=c("blue", "darkgreen", "red"), lwd=3)
# ============================
comp_results <- data.frame(
Method = c("GA", "DE", "PSO"),
Avg_Best = c(mean(GA_best), mean(DE_best), mean(PSO_best)),
Avg_Time = c(mean(GA_time), mean(DE_time), mean(PSO_time)),
Var_Best = c(var(GA_best), var(DE_best), var(PSO_best))
)
# ============================
# 0) Load required modules
# ============================
library(genalg)
library(DEoptim)
library(pso)
setwd("C:/Users/leodo/OneDrive/Escritorio/optimization/third delivery/OBA-II-Final-Proyect")
source("main_Data.R")
source("stations.R")
source("demand_supply.R")
source("graph_utils.R")
source("class_algorithms.R")
# ============================
# 1) Prepare graph + variables
# ============================
mapping_data <- make_mapping(all_names, N_OUTER)
outer_abbr <- mapping_data$outer_abbr
PT <- mapping_data$PT
g_data <- make_graph_with_weights(outer_abbr, PT)
g <- g_data$g
coords <- g_data$coords
minutes_per_block <- 2
E(g)$time_weight <- E(g)$weight * minutes_per_block
ds <- build_demand_supply(outer_abbr, N_OUTER, N_SURPLUS, PT)
d <- ds$demand
s <- ds$surplus
objective_numeric <- function(route, graph, PT, S0, demand, surplus,
minutes_per_block = 2, cost_per_min_eur = 0.6,
penalty_per_unit = 0.5) {
res <- objective_distance_extended(route, graph, PT, S0, demand, surplus,
minutes_per_block, cost_per_min_eur,
penalty_per_unit)
return(res$distance_blocks)
}
eval_fun <- function(route, ...) {
objective_numeric(
route,
graph = g, PT = PT, S0 = S0,
demand = d, surplus = s
)
}
# ============================
# 2) Wrapper for algorithms
# ============================
run_one_GA <- function() {
res <- ga_search(
outer_abbr = outer_abbr, fn = eval_fun,
graph = g, PT = PT, S0 = S0, demand = d,
surplus = s, penalty_per_unit = 0.5
)
list(best = res$best_cost, history = res$history, time = res$time_secs)
}
run_one_DE <- function() {
res <- de_search(
outer_abbr = outer_abbr, fn = eval_fun,
graph = g, PT = PT, S0 = S0, demand = d,
surplus = s, penalty_per_unit = 0.5
)
list(best = res$best_cost, history = res$history, time = res$time_secs)
}
run_one_PSO <- function() {
res <- pso_search(
outer_abbr = outer_abbr, fn = eval_fun,
graph = g, PT = PT, S0 = S0, demand = d,
surplus = s, penalty_per_unit = 0.5
)
list(best = res$best_cost, history = res$history, time = res$time_secs)
}
# NEW — TABU SEARCH
run_one_TABU <- function() {
res <- tabu_search(
par = sample(outer_abbr), fn = eval_fun,
graph = g, PT = PT, S0 = S0, demand = d,
surplus = s, penalty_per_unit = 0.5
)
list(best = res$best_cost, history = res$history, time = res$time_secs)
}
# ============================
# 3) RUN EXPERIMENTS
# ============================
Runs <- 10         # número de corridas
max_iters <- 25    # longitud estándar de convergencia
# Matrices de historial
GA_hist <- matrix(NA, nrow = Runs, ncol = max_iters)
DE_hist <- matrix(NA, nrow = Runs, ncol = max_iters)
PSO_hist <- matrix(NA, nrow = Runs, ncol = max_iters)
TABU_hist <- matrix(NA, nrow = Runs, ncol = max_iters)
# Vectores de resultados finales
GA_best <- numeric(Runs)
DE_best <- numeric(Runs)
PSO_best <- numeric(Runs)
TABU_best <- numeric(Runs)
GA_time <- numeric(Runs)
DE_time <- numeric(Runs)
PSO_time <- numeric(Runs)
TABU_time <- numeric(Runs)
# Padding
pad_history <- function(vec, max_len) {
n <- length(vec)
if (n >= max_len) return(vec[1:max_len])
c(vec, rep(tail(vec, 1), max_len - n))
}
# === MAIN LOOP ===
for (i in 1:Runs) {
cat(sprintf("\n--- RUN %d OF %d ---\n", i, Runs))
ga <- run_one_GA()
de <- run_one_DE()
pso <- run_one_PSO()
tabu <- run_one_TABU()   # NEW
GA_hist[i, ] <- pad_history(ga$history, max_iters)
DE_hist[i, ] <- pad_history(de$history, max_iters)
PSO_hist[i, ] <- pad_history(pso$history, max_iters)
TABU_hist[i, ] <- pad_history(tabu$history, max_iters)
GA_best[i] <- ga$best
DE_best[i] <- de$best
PSO_best[i] <- pso$best
TABU_best[i] <- tabu$best   # NEW
GA_time[i] <- ga$time
DE_time[i] <- de$time
PSO_time[i] <- pso$time
TABU_time[i] <- tabu$time   # NEW
}
# ============================
# 4) AGGREGATED STATISTICS
# ============================
avg_GA <- colMeans(GA_hist, na.rm = TRUE)
avg_DE <- colMeans(DE_hist, na.rm = TRUE)
avg_PSO <- colMeans(PSO_hist, na.rm = TRUE)
avg_TABU <- colMeans(TABU_hist, na.rm = TRUE)   # NEW
cat("\n===== AVERAGE BEST VALUES =====\n")
print(c(GA = mean(GA_best), DE = mean(DE_best), PSO = mean(PSO_best), TABU = mean(TABU_best)))
cat("\n===== AVERAGE TIMES (sec) =====\n")
print(c(GA = mean(GA_time), DE = mean(DE_time), PSO = mean(PSO_time), TABU = mean(TABU_time)))
# ============================
# 5) PLOT: Average Convergence
# ============================
plot(avg_GA, type="l", lwd=3, col="blue",
ylim = range(c(avg_GA, avg_DE, avg_PSO, avg_TABU)),
xlab="Iteration", ylab="Average best",
main="Average Convergence (GA, DE, PSO, TABU)")
lines(avg_DE, lwd=3, col="darkgreen")
lines(avg_PSO, lwd=3, col="red")
lines(avg_TABU, lwd=3, col="purple")   # NEW
legend("topright",
legend=c("GA", "DE", "PSO", "TABU"),
col=c("blue", "darkgreen", "red", "purple"),
lwd=3)
# ============================
# 6) BOXPLOT (Best Values)
# ============================
library(ggplot2)
df_box <- data.frame(
value = c(GA_best, DE_best, PSO_best, TABU_best),
method = factor(rep(c("GA", "DE", "PSO", "TABU"), each = Runs))
)
ggplot(df_box, aes(x = method, y = value, fill = method)) +
geom_boxplot(alpha = 0.8) +
theme_minimal() +
labs(
title = "Best Objective Values (GA, DE, PSO, TABU)",
x = "Algorithm",
y = "Objective Value"
) +
scale_fill_brewer(palette = "Set2")
# ============================
# 7) Summary Table
# ============================
comp_results <- data.frame(
Method = c("GA", "DE", "PSO", "TABU"),
Avg_Best = c(mean(GA_best), mean(DE_best), mean(PSO_best), mean(TABU_best)),
Avg_Time = c(mean(GA_time), mean(DE_time), mean(PSO_time), mean(TABU_time)),
Var_Best = c(var(GA_best), var(DE_best), var(PSO_best), var(TABU_best))
)
print(comp_results)
# =========================================================
# stats_algorithms.R — Statistical comparison of GA, DE, PSO
# Based on rubric point 6 (10% of grade)
# ==================
#install.packages("car")
library(car)
if (!all(c("GA_best", "DE_best", "PSO_best", "TABU_best") %in% ls())) {
stop("Error: Missing GA_best, DE_best, PSO_best or TABU_best vectors.")
}
# --- Create data frame for analysis ---
df <- data.frame(
value = c(GA_best, DE_best, PSO_best, TABU_best),
method = factor(rep(c("GA", "DE", "PSO", "TABU"),
each = length(GA_best)))
)
cat("\n===== DATA SUMMARY =====\n")
print(aggregate(value ~ method, df, summary))
# =========================================================
# 1) ANOVA TEST (Parametric)
# =========================================================
cat("\n===== ANOVA TEST =====\n")
anova_res <- aov(value ~ method, data = df)
print(summary(anova_res))
# Check normality of residuals
shapiro_res <- shapiro.test(residuals(anova_res))
cat("\nShapiro-Wilk normality test on residuals:\n")
print(shapiro_res)
# Check homogeneity of variances
levene_test <- car::leveneTest(value ~ method, df)
cat("\nLevene Test for homogeneity of variances:\n")
print(levene_test)
# =========================================================
# 2) KRUSKAL–WALLIS (non-parametric alternative)
# =========================================================
cat("\n===== KRUSKAL–WALLIS TEST =====\n")
kw <- kruskal.test(value ~ method, df)
print(kw)
# =========================================================
# 3) PAIRWISE WILCOXON TESTS (non-parametric pairwise)
# =========================================================
cat("\n===== PAIRWISE WILCOXON TESTS =====\n")
wilc <- pairwise.wilcox.test(df$value, df$method,
p.adjust.method = "bonferroni")
print(wilc)
# =========================================================
# 4) PAIRWISE t-TESTS (parametric pairwise)
# =========================================================
cat("\n===== PAIRWISE t-TESTS =====\n")
t <- pairwise.t.test(df$value, df$method,
p.adjust.method = "bonferroni")
print(t)
# =========================================================
# 5) AUTOMATIC INTERPRETATION
# =========================================================
cat("\n===== INTERPRETATION (AUTO-GENERATED) =====\n")
interpret <- function(anova_res, kw_res, t_res, wilcox_res) {
# Detect significance
sig_anova  <- summary(anova_res)[[1]][["Pr(>F)"]][1] < 0.05
sig_kw     <- kw_res$p.value < 0.05
cat("\n--- Global Tests ---\n")
if (sig_anova) {
cat("ANOVA: Significant differences detected between GA, DE, PSO (p < 0.05).\n")
} else {
cat("ANOVA: No significant differences between GA, DE, PSO.\n")
}
if (sig_kw) {
cat("Kruskal–Wallis: Significant differences detected (p < 0.05).\n")
} else {
cat("Kruskal–Wallis: No significant differences found.\n")
}
cat("\n--- Pairwise Comparisons ---\n")
# Pairwise t-tests
cat("\nPairwise t-tests (Bonferroni-adjusted):\n")
print(t_res$p.value)
# Pairwise Wilcoxon tests
cat("\nPairwise Wilcoxon tests (Bonferroni-adjusted):\n")
print(wilcox_res$p.value)
cat("\n--- Summary Interpretation ---\n")
# Determine best method
best_means <- tapply(df$value, df$method, mean)
best_method <- names(which.min(best_means))
cat(sprintf("Mean performance (lower = better): GA = %.2f, DE = %.2f, PSO = %.2f\n",
best_means["GA"], best_means["DE"], best_means["PSO"]))
cat(sprintf("Best overall method: %s\n", best_method))
cat("\nInterpretation:\n")
if (sig_kw | sig_anova) {
cat(sprintf("There are statistically significant differences between at least two algorithms.\n%s appears to outperform the others on average.\n",
best_method))
} else {
cat("There are NO statistically significant differences between the algorithms.\nThey perform similarly.\n")
}
}
interpret(anova_res, kw, t, wilc)
save_full_run <- function(
GA_best, DE_best, PSO_best, TABU_best,
GA_hist, DE_hist, PSO_hist, TABU_hist,
GA_time, DE_time, PSO_time, TABU_time,
console_log = "console_output.txt",
folder = NULL
) {
# ====== 1) Create timestamped folder ======
if (is.null(folder)) {
timestamp <- format(Sys.time(), "%Y-%m-%d_%H-%M-%S")
folder <- paste0("run_", timestamp)
}
dir.create(folder, showWarnings = FALSE)
# ====== 2) Save RData with all numeric results ======
save(
GA_best, DE_best, PSO_best, TABU_best,
GA_hist, DE_hist, PSO_hist, TABU_hist,
GA_time, DE_time, PSO_time, TABU_time,
file = file.path(folder, "algorithm_results.RData")
)
# ====== 3) Save CSV summary ======
summary_df <- data.frame(
run = 1:length(GA_best),
GA_best = GA_best,
DE_best = DE_best,
PSO_best = PSO_best,
TABU_best = TABU_best,
GA_time = GA_time,
DE_time = DE_time,
PSO_time = PSO_time,
TABU_time = TABU_time
)
write.csv(summary_df, file.path(folder, "algorithm_summary.csv"), row.names = FALSE)
# ====== 4) Move console log if exists ======
if (file.exists(console_log)) {
file.rename(console_log, file.path(folder, "console_output.txt"))
}
# ====== 5) Return path ======
message("Saved all results to folder: ", folder)
return(folder)
}
save_full_run <- function(
GA_best, DE_best, PSO_best, TABU_best,
GA_hist, DE_hist, PSO_hist, TABU_hist,
GA_time, DE_time, PSO_time, TABU_time,
console_log = "console_output.txt",
folder = NULL
) {
# ====== 1) Create timestamped folder ======
if (is.null(folder)) {
timestamp <- format(Sys.time(), "%Y-%m-%d_%H-%M-%S")
folder <- paste0("run_", timestamp)
}
dir.create(folder, showWarnings = FALSE)
# ====== 2) Save RData with all numeric results ======
save(
GA_best, DE_best, PSO_best, TABU_best,
GA_hist, DE_hist, PSO_hist, TABU_hist,
GA_time, DE_time, PSO_time, TABU_time,
file = file.path(folder, "algorithm_results.RData")
)
# ====== 3) Save CSV summary ======
summary_df <- data.frame(
run = 1:length(GA_best),
GA_best = GA_best,
DE_best = DE_best,
PSO_best = PSO_best,
TABU_best = TABU_best,
GA_time = GA_time,
DE_time = DE_time,
PSO_time = PSO_time,
TABU_time = TABU_time
)
write.csv(summary_df, file.path(folder, "algorithm_summary.csv"), row.names = FALSE)
# ====== 4) Move console log if exists ======
if (file.exists(console_log)) {
file.rename(console_log, file.path(folder, "console_output.txt"))
}
# ====== 5) Return path ======
message("Saved all results to folder: ", folder)
return(folder)
}
sink("console_output.txt")   # start capturing console
# --- run your experiments here ---
sink()                       # stop capturing console
save_full_run(
GA_best, DE_best, PSO_best, TABU_best,
GA_hist, DE_hist, PSO_hist, TABU_hist,
GA_time, DE_time, PSO_time, TABU_time
)
View(results)
View(summary_df)
a <- summary_df %>%
filter(Method, Distance_Blocks ,Cost_EUR)
a <- summary_df %>%
filter(Method, Distance_Blocks ,Cost_EUR)
# ================================================================
# final_algorithm.R — ejecución del modelo logístico (optimización por distancia con interpretación económica)
# ================================================================
# ==== 0) Preparación ====
rm(list = ls())
cat("\014")
set.seed(123)
library(igraph)
# ==== 1) Directorio base ====
setwd("C:/Users/leodo/OneDrive/Escritorio/optimization/third delivery/OBA-II-Final-Proyect")
# ==== 2) Cargar módulos ====
source("main_Data.R")
source("stations.R")
source("demand_supply.R")
source("graph_utils.R")
source("plot_utils.R")
source("class_algorithms.R")
# ==== 3) Generar el grafo ====
mapping_data <- make_mapping(all_names, N_OUTER)
outer_abbr <- mapping_data$outer_abbr
PT <- mapping_data$PT
g_data <- make_graph_with_weights(outer_abbr = outer_abbr, PT = PT)
g <- g_data$g
coords <- g_data$coords
minutes_per_block <- 2  # tiempo promedio por bloque
E(g)$time_weight <- E(g)$weight * minutes_per_block
# ==== 3.1) Añadir pesos en tiempo estimado (minutos) ====
minutes_per_block <- 2  # tiempo promedio por manzana
E(g)$time_weight <- E(g)$weight * minutes_per_block
# ==== 4) Preparar demanda y superávit ====
ds <- build_demand_supply(outer_abbr = outer_abbr,
N_OUTER = N_OUTER,
N_SURPLUS = N_SURPLUS,
PT = PT)
d <- ds$demand
s <- ds$surplus
cat("\nDemanda (d):\n"); print(d)
cat("\nSuperávit (s):\n"); print(s)
# ==== 5) Parámetros base (en unidades reales) ====
minutes_per_block <- 2        # tiempo medio por bloque (min)
cost_per_min_eur   <- 0.6     # coste operativo por minuto (€)
penalty_per_unit   <- 0.5     # penalización por unidad no servida (en bloques)
lambda <- NULL
# ==== 6) Función objetivo extendida ====
objective_distance_extended <- function(route, graph, PT, S0, demand, surplus,
minutes_per_block = 2,
cost_per_min_eur = 0.6,
penalty_per_unit = 0.5) {
full_route <- c(PT, route, PT)
total_distance <- 0
# ---- calcular distancia total en bloques ----
for (i in seq_along(full_route[-1])) {
from <- full_route[i]
to   <- full_route[i + 1]
total_distance <- total_distance + igraph::distances(graph, v = from, to = to)
}
# ---- penalización si el stock no cubre la demanda ----
total_demand <- sum(demand[demand > 0])
unmet <- max(0, total_demand - S0)
penalty_blocks <- unmet * penalty_per_unit
# ---- conversión a tiempo y costo ----
total_blocks <- as.numeric(total_distance + penalty_blocks)
total_minutes <- total_blocks * minutes_per_block
total_cost <- total_minutes * cost_per_min_eur
return(list(
distance_blocks = total_blocks,
time_minutes = total_minutes,
cost_eur = total_cost,
penalty_blocks = penalty_blocks
))
}
# ==== 7) Función wrapper para algoritmos ====
objective_numeric <- function(route, graph, PT, S0, demand, surplus,
minutes_per_block = 2, cost_per_min_eur = 0.6,
penalty_per_unit = 0.5) {
res <- objective_distance_extended(route, graph, PT, S0, demand, surplus,
minutes_per_block, cost_per_min_eur, penalty_per_unit)
return(res$distance_blocks)
}
# ==== 8) Visualizar red base ====
plot_network_time(g, PT = PT, coords = coords,
main = "Distribution network — edges weighted by estimated time (min)")
plot_network(g, PT = PT, demand = d, surplus = s, coords = coords,
main = "Distribution network — optimization by distance (blocks)")
# ==== 9) Ejecutar algoritmos ====
results <- run_all_algorithms(
outer_abbr,
FUN = function(route, ...) objective_numeric(route, graph=g, PT=PT, S0=S0, demand=d, surplus=s),
graph = g, PT = PT, S0 = S0, demand = d, surplus = s
)
